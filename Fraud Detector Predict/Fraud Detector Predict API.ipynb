{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to the AWS Fraud Detector Prediction API  \n",
    "#### Supervised fraud detection  \n",
    "-------\n",
    "- [Introduction](#Introduction)\n",
    "- [Setup](#Setup)\n",
    "- [Plan](#Plan)\n",
    "\n",
    "### Reviews \n",
    "https://github.com/aayush210789/Deception-Detection-on-Amazon-reviews-dataset\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "-------\n",
    "\n",
    "Amazon Fraud Detector is a fully managed service that makes it easy to identify potentially fraudulent online activities such as online payment fraud and the creation of fake accounts. Fraud Detector capitalizes on the latest advances in machine learning (ML) and 20 years of fraud detection expertise from AWS and Amazon.com to automatically identify potentially fraudulent activity so you can catch more fraud faster. \n",
    "\n",
    "In this notebook, we'll use the Amazon Fraud Detector Predict API to apply a Dector to sample data to identify potentially fraudlent envents. After running this notebook you should be able to: \n",
    "\n",
    "- Apply the Detector's \"predict\" function, to generate a model score and rule outcomes on data  \n",
    "\n",
    "If you would like to know more, please check out the [Fraud Detector's Documentation](https://docs.aws.amazon.com/frauddetector/). \n",
    "\n",
    "## Setup\n",
    "------\n",
    "First setup your AWS credentials so that Fraud Detector can store and access training data and supporting detector artifacts \n",
    "\n",
    "\n",
    "### Setting up AWS Credentials & Permissions\n",
    "\n",
    "https://docs.aws.amazon.com/frauddetector/latest/ug/set-up.html\n",
    "\n",
    "To use Amazon Fraud Detector, you have to set up permissions that allow access to the Amazon Fraud Detector console and API operations. You also have to allow Amazon Fraud Detector to perform tasks on your \n",
    "behalf and to access resources that you own.\n",
    "\n",
    "We recommend creating an AWS Identify and Access Management (IAM) user with access restricted to Amazon Fraud Detector operations and required permissions. You can add other permissions as needed.\n",
    "\n",
    "The following policies provide the required permission to use Amazon Fraud Detector:\n",
    "\n",
    "- *AmazonFraudDetectorFullAccessPolicy*  \n",
    "    Allows you to perform the following actions:  \n",
    "        - Access all Amazon Fraud Detector resources  \n",
    "        - List and describe all model endpoints in Amazon SageMaker  \n",
    "        - List all IAM roles in the account  \n",
    "        - List all Amazon S3 buckets  \n",
    "        - Allow IAM Pass Role to pass a role to Amazon Fraud Detector  \n",
    "\n",
    "- *AmazonS3FullAccess*  \n",
    "    Allows full access to Amazon S3. This is required to upload training files to S3.  \n",
    "\n",
    "\n",
    "\n",
    "## Plan\n",
    "------\n",
    "\n",
    "A *Detector* contains the model(s) and rule(s) detection logic for a particular type of fraud that you want to detect. We'll use the following 5 step process to plan a Fraud Detector: \n",
    "\n",
    "1. Detector Name  \n",
    "    - You'll need the name of the detector, you can look this up in the AFD console \n",
    "    \n",
    "2. Model Name   \n",
    "    - You'll need the active model name and Version used by the detector  \n",
    "    \n",
    "3. Call Prediction API \n",
    "    - You'll need to specify the number of records to predict \n",
    "\n",
    "4. Score Threshold\n",
    "    - the score threshold is the cut off where above the threshold you'll call the record fraud else it's legit \n",
    "    - this is used to create confusion matrix (TP,FP,TN,FN)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:90% }</style>\"))\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# -- dask for parallelisim -- \n",
    "import dask \n",
    "\n",
    "# -- standard stuff -- \n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# -- AWS stuff -- \n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# -- sklearn --\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, roc_auc_score\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AWS Fraud Detector Client \n",
    "------\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/frauddetector.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- fraud detector client --\n",
    "client = boto3.client('frauddetector')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detector, Model, and Identifiers \n",
    "-----\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Detector, Model and Versions </strong>\n",
    "\n",
    "- DETECTOR_NAME & VERSION coresponds to the name and version of your deployed Fraud Detector  \n",
    "- MODEL_NAME & VERSION coresponds to the name and version of the model deployed with your Fraud Detector   \n",
    "- FRAUD_LABEL is useful if you are <b> comparing performance of your detector's </b>predictions to known frauds this is optional   \n",
    "- EMAIL_ADDRESS is used as a key to identify your prediction infrences, you can look up a specific infrence in console by seraching for a specific email address. This maps to the <b>email address field</b> in your file you are predicting on. \n",
    "- S3_FILE this is the url of the S3 file you wish to apply your detector to.   \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTOR_NAME = \"your_fraud_detector_name\"\n",
    "DETECTOR_VER  = '1.0'\n",
    "\n",
    "MODEL_NAME    = \"your_model_name\"\n",
    "MODEL_VER     = '1.0'\n",
    "\n",
    "# -- if fraud label exists -- \n",
    "FRAUD_LABEL   = \"your_target_field\"\n",
    "\n",
    "# -- use email as the identifier for predictions\n",
    "EMAIL_ADDRESS = \"email_address\"\n",
    "\n",
    "# -- input file of data to be scored -- \n",
    "S3_FILE       = \"s3://your-bucket-name/your-file-to-predict.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data to be Scored \n",
    "-----\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Check the first 5 Records </strong>\n",
    "\n",
    "- Does your data look correct? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(S3_FILE)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detector Details \n",
    "-----\n",
    "\n",
    "This simply displays details about your detector, the main thing you want to see is that your Detector's statsus is 'ACTIVE' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- details on your detector -- \n",
    "response = client.describe_detector(\n",
    "    detectorId = DETECTOR_NAME ,\n",
    ")\n",
    "response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Info\n",
    "------\n",
    "\n",
    "This section will display the score threshold table that you see in console. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- model performance summary -- \n",
    "auc = eval(client.describe_model_versions(\n",
    "    modelId= MODEL_NAME,\n",
    "    modelVersionNumber=MODEL_VER,\n",
    "    modelType='ONLINE_FRAUD_INSIGHTS',\n",
    "    maxResults=10\n",
    ")['modelVersionDetails'][0]['trainingMetrics']['auc'])\n",
    "\n",
    "thr = eval(client.describe_model_versions(\n",
    "    modelId= MODEL_NAME,\n",
    "    modelVersionNumber=MODEL_VER,\n",
    "    modelType='ONLINE_FRAUD_INSIGHTS',\n",
    "    maxResults=10\n",
    ")['modelVersionDetails'][0]['trainingMetrics']['thresholds'])\n",
    "\n",
    "fpr = eval(client.describe_model_versions(\n",
    "    modelId= MODEL_NAME,\n",
    "    modelVersionNumber=MODEL_VER,\n",
    "    modelType='ONLINE_FRAUD_INSIGHTS',\n",
    "    maxResults=10\n",
    ")['modelVersionDetails'][0]['trainingMetrics']['fpr'])\n",
    "\n",
    "tpr = eval(client.describe_model_versions(\n",
    "    modelId= MODEL_NAME,\n",
    "    modelVersionNumber=MODEL_VER,\n",
    "    modelType='ONLINE_FRAUD_INSIGHTS',\n",
    "    maxResults=10\n",
    ")['modelVersionDetails'][0]['trainingMetrics']['tpr'])\n",
    "\n",
    "precision = eval(client.describe_model_versions(\n",
    "    modelId= MODEL_NAME,\n",
    "    modelVersionNumber=MODEL_VER,\n",
    "    modelType='ONLINE_FRAUD_INSIGHTS',\n",
    "    maxResults=10\n",
    ")['modelVersionDetails'][0]['trainingMetrics']['precision'])\n",
    "precision\n",
    "\n",
    "df_model = pd.DataFrame(list(zip(thr, fpr, tpr, precision)), columns=['thr','fpr', 'tpr', 'precision'])\n",
    "model_stat = df_model.round(decimals=2)               \n",
    "m = model_stat.loc[model_stat.groupby([\"fpr\"])[\"thr\"].idxmax()] \n",
    "def make_rule(x):\n",
    "    return \"\\'score > \" + str(x) + \"\\'\"\n",
    "    \n",
    "m['score threshold'] = m['thr'].apply(lambda x: make_rule(x))\n",
    "\n",
    "print (\" --- score thresholds 1% to 10% --- \")\n",
    "print(m[[\"fpr\", \"tpr\", \"score threshold\"]].loc[(m['fpr'] > 0.0 ) & (m['fpr'] <= 0.1)].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Thresholds \n",
    "\n",
    "----\n",
    "Identify a score threshold based on the false positive rate. \n",
    "\n",
    "- most operatoins operate at a 1% - 4% false positive ratio \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> False Positive & True Positive Rates </strong>\n",
    "\n",
    "- false positive rate - is the % of events incorrectly identified as fraud for a given score threshold \n",
    "\n",
    "- true positive rate - is the % of events correctly identified at a given score threshold \n",
    "\n",
    "- identify a score threshold that coresponds to your false positive rate, if in doubt start with the 1% FPR\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- update threshold -- \n",
    "score_threshold = 800\n",
    "\n",
    "# -- FPR based on score threshold above -- \n",
    "fpr_threshold = list(m.iloc[(m['thr']- score_threshold).abs().argsort()[:1]]['fpr'])[0]\n",
    "tpr_threshold = list(m.iloc[(m['thr']- score_threshold).abs().argsort()[:1]]['tpr'])[0]\n",
    "\n",
    "print(\"At the score of \" + str(score_threshold) + \" the model will idenitify \" + str(tpr_threshold * 100) + \"% of fraudulent events with a \" + str(fpr_threshold*100)+\"% false rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model ROC Plot \n",
    "------\n",
    "The following charts show the model Area Under the Curve at the Specified Score threshold and coresponding False Positive Rate and True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10),)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "fig.suptitle( MODEL_NAME + ' ROC Chart \\n\\n score > ' + \n",
    "           str(score_threshold) + ' = FPR @' + \n",
    "           str(fpr_threshold*100) + '% AND TPR @' + \n",
    "           str(tpr_threshold*100) +'%') \n",
    "ax1.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.3f)' % auc)\n",
    "ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "#plt.xlabel('False Positive Rate')\n",
    "#plt.ylabel('True Positive Rate')\n",
    "\n",
    "ax1.legend(loc=\"lower right\",fontsize=12)\n",
    "ax1.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "ax1.axvline(x = fpr_threshold ,linewidth=2, color='r')\n",
    "ax1.axhline(y = tpr_threshold ,linewidth=2, color='r')\n",
    "\n",
    "\n",
    "ax2.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.3f)' % auc)\n",
    "ax2.set_xlim([0, 0.2])\n",
    "\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "\n",
    "ax2.legend(loc=\"lower right\",fontsize=12)\n",
    "ax2.axhline(y = tpr_threshold ,linewidth=2, color='r')\n",
    "ax2.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Setup for Scoring\n",
    "-----\n",
    "The following function returns model variables  \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Model Variables </strong>\n",
    "\n",
    "- pass just the variables needed for the detector to score \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_variables(MODEL_NAME):\n",
    "    \"\"\" return list of variables used by a model \n",
    "    \n",
    "    \"\"\"\n",
    "    response = client.get_models(\n",
    "    modelType='ONLINE_FRAUD_INSIGHTS',\n",
    "    modelId= MODEL_NAME)\n",
    "    model_variables = []\n",
    "\n",
    "    for v in response['models'][0]['modelVariables']:\n",
    "        model_variables.append(v['name'])\n",
    "    return model_variables\n",
    "\n",
    "model_variables = get_model_variables(MODEL_NAME)\n",
    "print(\"\\n -- model variables -- \")\n",
    "print(model_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Predictions  \n",
    "-----\n",
    "The following applies the get_prediction to records   \n",
    "\n",
    "<i> Note: this uses the Dask backend to parallelize the prediction calls. </i>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong>get_prediction </strong>\n",
    "\n",
    "- Specify the number of records to score, you change the record_count to a specific number if you want to just predict on say 100 records, by default it assumes you want to apply predicitons to the whole dataset. \n",
    "- Once completed conver json to a pandas dataframe, appends any existing labels\n",
    "- Analyze based on score threshold for a particular false positive rate FPR\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_count = df.shape[0]\n",
    "start = time.time()\n",
    "\n",
    "@dask.delayed\n",
    "def _predict(record):\n",
    "    stime = time.time()\n",
    "    try:\n",
    "        pred  = client.get_prediction(detectorId=DETECTOR_NAME, detectorVersionId='1.0', eventId = record[EMAIL_ADDRESS], eventAttributes = record)\n",
    "        etime = time.time()\n",
    "        record['outcome'] = pred['outcomes'][0]\n",
    "        record['status'] = pred['ResponseMetadata']['HTTPStatusCode']\n",
    "        record['score']  = pred['modelScores'][0]['scores'][MODEL_NAME + '_insightscore']\n",
    "        record['score_ms'] = ((etime - stime)*1000)\n",
    "        return record\n",
    "    except:\n",
    "        pred  = client.get_prediction(detectorId=DETECTOR_NAME, detectorVersionId='1.0', eventId = record[EMAIL_ADDRESS], eventAttributes = record)\n",
    "        etime = time.time()\n",
    "        record['outcome'] = '-- failed --'\n",
    "        record['status']  = pred['ResponseMetadata']['HTTPStatusCode']\n",
    "        record['score']   =  -1 \n",
    "        record['score_ms'] = ((etime - stime)*1000)\n",
    "        return record\n",
    "\n",
    "predict_data  = df[model_variables].head(record_count).astype(str).to_dict(orient='records')\n",
    "predict_score = []\n",
    "\n",
    "i=0\n",
    "for record in predict_data:\n",
    "    clear_output(wait=True)\n",
    "    rec = dask.delayed(_predict)(record)\n",
    "    predict_score.append(rec)\n",
    "    i += 1\n",
    "    print(\"current progress: \", round((i/record_count)*100,2), \"%\" )\n",
    "    \n",
    "\n",
    "predict_recs = dask.compute(*predict_score)\n",
    "\n",
    "# Calculate time taken and print results\n",
    "time_taken = time.time() - start\n",
    "tps = len(predict_recs) / time_taken\n",
    "\n",
    "print ('Process took %0.2f seconds' %time_taken)\n",
    "print ('Scored %d records' %len(predict_recs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at your predictions\n",
    "-----\n",
    "Each record will have a score, the time (ms) it took to score it, the outcome and if a label was provided the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame.from_dict(predict_recs, orient='columns')\n",
    "if FRAUD_LABEL:\n",
    "    predictions[FRAUD_LABEL] = df[FRAUD_LABEL].head(record_count)\n",
    "    all_variables = ['score', 'score_ms', 'outcome', FRAUD_LABEL] + model_variables\n",
    "else:\n",
    "    all_variables = ['score', 'score_ms', 'outcome'] + model_variables\n",
    "\n",
    "predictions[all_variables].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions.loc[predictions['score'] >= score_threshold, 'y_pred'] = 1\n",
    "predictions.loc[predictions['score'] < score_threshold, 'y_pred'] = 0\n",
    "accuracy_score(predictions[FRAUD_LABEL], predictions['y_pred'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally Write Predictions to File\n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> Write Predictions </strong>\n",
    "\n",
    "- You can write your prediction dataset to a CSV to manually review predictions\n",
    "- Simply add a cell below and copy the code below\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# -- optionally write predictions to a CSV file -- \n",
    "predictions.to_csv(MODEL_NAME + \".csv\", index=False)\n",
    "# -- or to a XLS file \n",
    "predictions.to_excel(MODEL_NAME + \".xlsx\", index=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION\n",
    "------\n",
    "The following section requires FRAUD_LABEL to be set to a Column value in the prediction dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Distribution\n",
    "-----\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Separationg </strong>\n",
    "\n",
    "- typically we recomend 1 - 4% false positive rate (FPR) but this is totally business dependant \n",
    "- the table below will help you identify a score threshold to evaluate your model with. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if FRAUD_LABEL:\n",
    "    # -- assign predictions based on threshold --\n",
    "    predictions.loc[predictions['score'].astype(float) > score_threshold, \"predicted_fraud\" ] = 1\n",
    "    predictions.loc[predictions['score'].astype(float) <= score_threshold, \"predicted_fraud\" ] = 0\n",
    "\n",
    "\n",
    "    fraud = predictions.loc[predictions[FRAUD_LABEL] == 1 ]\n",
    "    legit = predictions.loc[predictions[FRAUD_LABEL] == 0 ]\n",
    "\n",
    "    bins = np.linspace(0, 1000, 100)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.hist(legit['score'].astype(float) , bins, alpha=1, density=True, label='Normal')\n",
    "    plt.hist(fraud['score'].astype(float) , bins, alpha=0.5, density=True, label='Fraud')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"AWS Fraud Detector Score Distribution\")\n",
    "    plt.xlabel(\"score\")\n",
    "    plt.ylabel(\"Percentage of transactions (%)\");\n",
    "    plt.axvline(x = score_threshold ,linewidth=4, color='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS Fraud Detector Prediction Classification \n",
    "\n",
    "-----\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> TP/TN/FP/FN </strong>\n",
    "\n",
    "- True Positive (TP) - correctly identified fraud events  \n",
    "- True Negative (TN) - correctly identified legitimate events  \n",
    "- False Positive (FP) - legitmate events incorrectly identified as fraud   \n",
    "- False Negative (FP) - fraudulent events incorrectly identified as legimate  \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FRAUD_LABEL:\n",
    "    tp = predictions.loc[(predictions[FRAUD_LABEL] == 1) & (predictions['predicted_fraud'].astype(float) == 1)]\n",
    "    tn = predictions.loc[(predictions[FRAUD_LABEL] == 0) & (predictions['predicted_fraud'].astype(float) == 0)]\n",
    "    fp = predictions.loc[(predictions[FRAUD_LABEL] == 0) & (predictions['predicted_fraud'].astype(float) == 1)]\n",
    "    fn = predictions.loc[(predictions[FRAUD_LABEL] == 1) & (predictions['predicted_fraud'].astype(float) == 0)]\n",
    "\n",
    "    bins = np.linspace(0, 1000, 100)\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.hist(tn['score'].astype(float), bins, alpha=1, density=True, label='True Legit')\n",
    "    plt.hist(tp['score'].astype(float), bins, alpha=0.5, density=True, label='True Fraud')\n",
    "\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"AWS Fraud Detector Score Distribution \\n Correct Classifications\")\n",
    "    plt.xlabel(\"score\")\n",
    "    plt.ylabel(\"Event %\");\n",
    "    plt.axvline(x = score_threshold ,linewidth=4, color='r')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.hist(fp['score'].astype(float), bins, alpha=1, density=True, label='False Positive')\n",
    "    plt.hist(fn['score'].astype(float), bins, alpha=0.5, density=True, label='Missed Fraud')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"AWS Fraud Detector Score Distribution \\n Missclassifications\")\n",
    "\n",
    "    plt.xlabel(\"score\")\n",
    "    plt.ylabel(\"Event %\");\n",
    "    plt.axvline(x = score_threshold ,linewidth=4, color='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Confusion Matrix \n",
    "----\n",
    "the following sports a typical confusion matrix of actual vs. predicted fraud / not fraud, assuming the fraud label exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FRAUD_LABEL:\n",
    "    confusion_matrix = pd.crosstab(predictions[FRAUD_LABEL], predictions['predicted_fraud'], rownames=['Actual'], colnames=['Predicted'],)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    sns.set(font_scale=1.5)\n",
    "    ax = sns.heatmap(confusion_matrix, annot=True, fmt='g',cmap=\"YlGnBu\")\n",
    "    plt.ylabel('Actual Fraud')\n",
    "    plt.xlabel('Predicted Fraud')\n",
    "    ax.xaxis.set_ticks_position('top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
